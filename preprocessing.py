import pandas as pd
import pymongo
from pymongo import MongoClient
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup
import logging
from datetime import datetime

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TextPreprocessor:
    """Classe pour le pr√©traitement des textes"""
    
    def __init__(self):
        self.setup_nltk()
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        
    def setup_nltk(self):
        """T√©l√©charge les ressources NLTK n√©cessaires"""
        required_resources = [
            ('tokenizers/punkt', 'punkt'),
            ('tokenizers/punkt_tab', 'punkt_tab'),
            ('corpora/stopwords', 'stopwords'),
            ('corpora/wordnet', 'wordnet'),
            ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),
            ('corpora/omw-1.4', 'omw-1.4')
        ]
        
        for resource_path, resource_name in required_resources:
            try:
                nltk.data.find(resource_path)
            except LookupError:
                logger.info(f"T√©l√©chargement de {resource_name}...")
                try:
                    nltk.download(resource_name, quiet=True)
                except Exception as e:
                    logger.warning(f"Impossible de t√©l√©charger {resource_name}: {e}")
        
        logger.info("Configuration NLTK termin√©e")
    
    def remove_html_tags(self, text):
        """Supprime les balises HTML"""
        soup = BeautifulSoup(text, "html.parser")
        return soup.get_text()
    
    def remove_urls(self, text):
        """Supprime les URLs"""
        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        return url_pattern.sub('', text)
    
    def remove_special_chars(self, text):
        """Supprime les caract√®res sp√©ciaux mais garde les espaces"""
        # Garde seulement les lettres, chiffres et espaces
        return re.sub(r'[^a-zA-Z0-9\s]', '', text)
    
    def remove_punctuation_and_digits(self, text):
        """Supprime la ponctuation et les chiffres"""
        # Supprime la ponctuation
        text = text.translate(str.maketrans('', '', string.punctuation))
        # Supprime les chiffres
        text = re.sub(r'\d+', '', text)
        return text
    
    def remove_stopwords(self, tokens):
        """Supprime les mots vides"""
        return [token for token in tokens if token.lower() not in self.stop_words]
    
    def lemmatize_tokens(self, tokens):
        """Applique la lemmatisation"""
        return [self.lemmatizer.lemmatize(token.lower()) for token in tokens]
    
    def preprocess_text(self, text):
        """Pipeline complet de pr√©traitement"""
        if not text or pd.isna(text):
            return {
                'original_text': '',
                'cleaned_text': '',
                'tokens': [],
                'processed_text': '',
                'word_count': 0
            }
        
        # √âtape 1: Conversion en string et minuscules
        text = str(text).lower()
        original_text = text
        
        # √âtape 2: Suppression des balises HTML
        text = self.remove_html_tags(text)
        
        # √âtape 3: Suppression des URLs
        text = self.remove_urls(text)
        
        # √âtape 4: Suppression des caract√®res sp√©ciaux
        text = self.remove_special_chars(text)
        
        # √âtape 5: Suppression de la ponctuation et des chiffres
        text = self.remove_punctuation_and_digits(text)
        
        # √âtape 6: Suppression des espaces multiples
        text = re.sub(r'\s+', ' ', text).strip()
        
        cleaned_text = text
        
        # √âtape 7: Tokenisation
        tokens = word_tokenize(text)
        
        # √âtape 8: Suppression des mots vides
        tokens = self.remove_stopwords(tokens)
        
        # √âtape 9: Lemmatisation
        tokens = self.lemmatize_tokens(tokens)
        
        # √âtape 10: Suppression des tokens vides
        tokens = [token for token in tokens if token.strip() and len(token) > 1]
        
        # Reconstitution du texte trait√©
        processed_text = ' '.join(tokens)
        
        return {
            'original_text': original_text,
            'cleaned_text': cleaned_text,
            'tokens': tokens,
            'processed_text': processed_text,
            'word_count': len(tokens)
        }

def connect_mongodb():
    """Connexion √† MongoDB"""
    try:
        client = MongoClient("mongodb://localhost:27017/")
        db = client["harcelement"]
        collection = db["posts"]
        logger.info("Connexion MongoDB √©tablie")
        return client, db, collection
    except Exception as e:
        logger.error(f"Erreur MongoDB: {e}")
        return None, None, None

def get_documents_to_process(collection, batch_size=100):
    """R√©cup√®re les documents √† traiter par batch"""
    try:
        cursor = collection.find({"processed": False}).limit(batch_size)
        documents = list(cursor)
        logger.info(f"Documents r√©cup√©r√©s pour traitement: {len(documents)}")
        return documents
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration documents: {e}")
        return []

def update_document_with_preprocessing(collection, doc_id, preprocessing_result):
    """Met √† jour un document avec les r√©sultats du pr√©traitement"""
    try:
        update_data = {
            "original_text": preprocessing_result['original_text'],
            "cleaned_text": preprocessing_result['cleaned_text'],
            "tokens": preprocessing_result['tokens'],
            "processed_text": preprocessing_result['processed_text'],
            "word_count": preprocessing_result['word_count'],
            "processed": True,
            "preprocessing_date": datetime.utcnow()
        }
        
        result = collection.update_one(
            {"_id": doc_id},
            {"$set": update_data}
        )
        
        return result.modified_count > 0
    except Exception as e:
        logger.error(f"Erreur mise √† jour document {doc_id}: {e}")
        return False

def process_documents(collection, preprocessor):
    """Traite tous les documents non trait√©s"""
    total_processed = 0
    batch_size = 100
    
    while True:
        # R√©cup√®re un batch de documents
        documents = get_documents_to_process(collection, batch_size)
        
        if not documents:
            break
        
        batch_processed = 0
        
        for doc in documents:
            try:
                # Pr√©traitement du texte
                text_field = doc.get('text', '')
                preprocessing_result = preprocessor.preprocess_text(text_field)
                
                # Mise √† jour du document
                if update_document_with_preprocessing(collection, doc['_id'], preprocessing_result):
                    batch_processed += 1
                    
                    # Log de progression
                    if batch_processed % 50 == 0:
                        logger.info(f"Trait√©s dans ce batch: {batch_processed}")
                        
            except Exception as e:
                logger.error(f"Erreur traitement document {doc.get('_id')}: {e}")
                continue
        
        total_processed += batch_processed
        logger.info(f"Batch termin√©: {batch_processed} documents trait√©s")
        
        # Si le batch n'est pas complet, on a termin√©
        if len(documents) < batch_size:
            break
    
    logger.info(f"Pr√©traitement termin√©: {total_processed} documents trait√©s au total")
    return total_processed

def verify_preprocessing(collection):
    """V√©rifie les r√©sultats du pr√©traitement"""
    try:
        total = collection.count_documents({})
        processed = collection.count_documents({"processed": True})
        
        print(f"\n=== STATISTIQUES DU PR√âTRAITEMENT ===")
        print(f"Total documents: {total}")
        print(f"Documents trait√©s: {processed}")
        print(f"Documents non trait√©s: {total - processed}")
        
        # Statistiques sur les mots
        pipeline = [
            {"$match": {"processed": True}},
            {"$group": {
                "_id": None,
                "avg_word_count": {"$avg": "$word_count"},
                "min_word_count": {"$min": "$word_count"},
                "max_word_count": {"$max": "$word_count"}
            }}
        ]
        
        stats = list(collection.aggregate(pipeline))
        if stats:
            stat = stats[0]
            print(f"\n=== STATISTIQUES DES MOTS ===")
            print(f"Nombre moyen de mots: {stat['avg_word_count']:.2f}")
            print(f"Minimum de mots: {stat['min_word_count']}")
            print(f"Maximum de mots: {stat['max_word_count']}")
        
        # √âchantillons de textes trait√©s
        print(f"\n=== √âCHANTILLONS ===")
        samples = collection.find({"processed": True}).limit(3)
        
        for i, doc in enumerate(samples, 1):
            print(f"\n√âchantillon {i}:")
            print(f"Label: {doc.get('label')}")
            print(f"Texte original: {doc.get('original_text', '')[:100]}...")
            print(f"Texte nettoy√©: {doc.get('cleaned_text', '')[:100]}...")
            print(f"Texte trait√©: {doc.get('processed_text', '')[:100]}...")
            print(f"Nombre de mots: {doc.get('word_count', 0)}")
            
    except Exception as e:
        logger.error(f"Erreur v√©rification: {e}")

def reset_processing_status(collection):
    """Remet √† z√©ro le statut de traitement (utile pour les tests)"""
    try:
        result = collection.update_many(
            {},
            {"$unset": {
                "original_text": "",
                "cleaned_text": "",
                "tokens": "",
                "processed_text": "",
                "word_count": "",
                "preprocessing_date": ""
            },
            "$set": {"processed": False}}
        )
        logger.info(f"Statut de traitement remis √† z√©ro pour {result.modified_count} documents")
        return result.modified_count
    except Exception as e:
        logger.error(f"Erreur reset: {e}")
        return 0

def main():
    """Fonction principale"""
    print("=== PR√âTRAITEMENT DES DONN√âES TEXTUELLES ===\n")
    
    # Connexion MongoDB
    client, db, collection = connect_mongodb()
    if collection is None:
        print("‚ùå Impossible de se connecter √† MongoDB")
        return
    
    try:
        # Initialisation du pr√©processeur
        print("üîß Initialisation du pr√©processeur...")
        preprocessor = TextPreprocessor()
        
        # Option pour reset (utile en d√©veloppement)
        reset_choice = input("Voulez-vous remettre √† z√©ro le traitement? (y/N): ")
        if reset_choice.lower() == 'y':
            print("üîÑ Remise √† z√©ro du statut de traitement...")
            reset_processing_status(collection)
        
        # Traitement des documents
        print("\nüìù D√©but du pr√©traitement...")
        processed_count = process_documents(collection, preprocessor)
        
        if processed_count > 0:
            print(f"‚úÖ Pr√©traitement termin√©: {processed_count} documents trait√©s")
        else:
            print("‚ÑπÔ∏è Aucun document √† traiter (tous d√©j√† trait√©s)")
        
        # V√©rification des r√©sultats
        print("\nüîç V√©rification des r√©sultats:")
        verify_preprocessing(collection)
        
        print("\n‚úÖ PR√âTRAITEMENT TERMIN√â AVEC SUCC√àS!")
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©rale: {e}")
        logger.error(f"Erreur g√©n√©rale: {e}")
    
    finally:
        if client:
            client.close()

if __name__ == "__main__":
    main()