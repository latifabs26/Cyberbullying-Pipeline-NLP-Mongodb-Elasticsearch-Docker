# Projet d'Analyse du HarcÃ¨lement en Ligne

## ğŸ¯ Objectif

Ce projet vise Ã  analyser les donnÃ©es de harcÃ¨lement en ligne en utilisant une stack complÃ¨te de data science : Python, MongoDB, Elasticsearch et Kibana. L'objectif est de crÃ©er un pipeline complet de traitement de donnÃ©es textuelles, depuis l'ingestion jusqu'Ã  la visualisation.

## ğŸ—ï¸ Architecture

```
Dataset CSV â†’ MongoDB â†’ PrÃ©traitement â†’ NLP â†’ Elasticsearch â†’ Kibana
```

### Technologies utilisÃ©es

- **Python** : Traitement des donnÃ©es, NLP, ingestion
- **MongoDB** : Base de donnÃ©es NoSQL pour le stockage initial
- **Elasticsearch** : Moteur de recherche et d'indexation
- **Kibana** : Visualisation et tableaux de bord
- **Docker** : Containerisation de l'environnement

## ğŸ“‹ PrÃ©requis

- Docker et Docker Compose installÃ©s
- Python 3.8+ (si exÃ©cution en local)
- Au moins 4GB de RAM disponible pour Elasticsearch

## ğŸš€ Installation et Configuration

### 1. Cloner le projet et prÃ©parer l'environnement

```bash
git clone <votre-repo>
cd cyberbullying-analysis
```

### 2. Lancer l'infrastructure avec Docker Compose

```bash
# Lancer tous les services
docker-compose up -d
#Or
make up

# VÃ©rifier que tous les services sont opÃ©rationnels
docker-compose ps
```

### 3. AccÃ¨s aux services

- **MongoDB** : `localhost:27017`
- **Mongo Express** : `http://localhost:8081` (admin/admin)
- **Elasticsearch** : `http://localhost:9200`
- **Kibana** : `http://localhost:5601`

### 4. Installation des dÃ©pendances Python

```bash
# CrÃ©er un environnement virtuel
venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ“Š Dataset

### Source
- **Dataset principal** : [Cyberbullying and Harassment Detection](https://www.kaggle.com/datasets/saifulislam7/cyberbullying-and-harassmentdetection-using-ml
)
- **Format** : CSV
- **Contenu** : Messages textuels avec labels de classification

### PrÃ©paration
1. TÃ©lÃ©charger le dataset depuis Kaggle
2. Renommer le fichier en `bullying.csv`
3. Placer le fichier dans le rÃ©pertoire racine du projet

## ğŸ”„ Pipeline de Traitement

### Ã‰tape 1 : Ingestion des donnÃ©es (data_loader.py)

```bash
python data_loader.py
#Or
make ingest 
```

**FonctionnalitÃ©s :**
- Lecture du fichier CSV avec pandas
- DÃ©tection automatique des colonnes (text, label, type)
- Nettoyage des donnÃ©es vides
- Insertion dans MongoDB avec mÃ©tadonnÃ©es :
  - `id_post` : Identifiant unique UUID
  - `text` : Contenu du message
  - `label` : Classification (harcÃ¨lement ou non)
  - `type` : Type de harcÃ¨lement
  - `created_at` : Timestamp d'insertion
  - `source` : Source des donnÃ©es

### Ã‰tape 2 : PrÃ©traitement (preprocessing.py)

```bash
python preprocessing.py
#Or
make preprocess
```

**Transformations appliquÃ©es :**
- Conversion en minuscules
- Suppression des balises HTML
- Suppression des URLs
- Suppression des caractÃ¨res spÃ©ciaux et ponctuation
- Suppression des chiffres
- Suppression des mots vides (stopwords)
- Lemmatisation avec NLTK
- Tokenisation

**Champs ajoutÃ©s :**
- `original_text` : Texte original
- `cleaned_text` : Texte nettoyÃ©
- `tokens` : Liste des tokens
- `processed_text` : Texte final traitÃ©
- `word_count` : Nombre de mots

### Ã‰tape 3 : Traitement NLP (nlp_pipeline.py)

```bash
python nlp_pipeline.py
#or
make nlp
```

**Analyses effectuÃ©es :**
- **DÃ©tection de langue** : Utilisation de `langdetect`
- **Analyse de sentiment** : Double approche avec TextBlob et VADER
- **Consensus de sentiment** : Algorithme de fusion des rÃ©sultats

**Champs ajoutÃ©s :**
- `language_detection` : Langue dÃ©tectÃ©e et niveau de confiance
- `textblob_sentiment` : RÃ©sultats TextBlob (polaritÃ©, subjectivitÃ©)
- `vader_sentiment` : RÃ©sultats VADER (scores dÃ©taillÃ©s)
- `sentiment_consensus` : Sentiment final et score de confiance

### Ã‰tape 4 : Indexation Elasticsearch (es_ingest.py)

```bash
python es_ingest.py
#Or
make index
```
### Run the complete Pipeline 
```bash
make all
```

**FonctionnalitÃ©s :**
- CrÃ©ation de l'index `harcelement_posts`
- Mapping optimisÃ© pour la recherche et l'analyse
- Transfert des donnÃ©es enrichies depuis MongoDB
- Gestion des erreurs et reprise en cas d'Ã©chec

### Ã‰tape 5 : Visualisation Kibana

AccÃ©der Ã  Kibana via `http://localhost:5601` et importer les dashboards prÃ©-configurÃ©s.

## ğŸ“ Structure du Projet

```
cyberbullying-analysis/
â”œâ”€â”€ data_loader.py              # Ã‰tape 1: Ingestion des donnÃ©es
â”œâ”€â”€ preprocessing.py            # Ã‰tape 2: PrÃ©traitement des textes
â”œâ”€â”€ nlp_pipeline.py            # Ã‰tape 3: Traitement NLP
â”œâ”€â”€ es_ingest.py               # Ã‰tape 4: Indexation Elasticsearch
â”œâ”€â”€ tests/                     # Tests unitaires
â”‚   â”œâ”€â”€ test_data_loader.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â””â”€â”€ test_nlp_pipeline.py
â”œâ”€â”€ docker-compose.yml         # Configuration Docker
â”œâ”€â”€ Dockerfile                 # Image Python personnalisÃ©e
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ README.md                  # Documentation
â”œâ”€â”€ Makefile                   # Makefile pour automatiser les commandes Docker et scripts Python
â””â”€â”€ bullying.csv              # Dataset (Ã  tÃ©lÃ©charger)
```

## ğŸ§ª Tests

### ExÃ©cution des tests unitaires

```bash
# Tous les tests
python -m pytest tests/ -v

# Tests spÃ©cifiques
python -m pytest tests/test_data_loader.py -v
python -m pytest tests/test_preprocessing.py -v
python -m pytest tests/test_nlp_pipeline.py -v
```

### Tests de couverture

```bash
python -m pytest tests/ --cov=. --cov-report=html
```

## ğŸ“Š Visualisations Kibana

### Dashboards disponibles

1. **ğŸ—£ï¸ RÃ©partition des Langues**
   - Type : Pie chart
   - But : Montrer les langues les plus frÃ©quentes dans les publications
   - Exemple : en, fr, other

2. **ğŸš¨ Types de HarcÃ¨lement**
   - Type : Diagramme en barres
   - But : Identifier les types de harcÃ¨lement les plus courants

3. **ğŸ˜Š Analyse des Sentiments**
   - Type : Donut
   - But : Visualiser la distribution des sentiments (positive, neutral, negative)

4. **ğŸ“‰ Posts NÃ©gatifs** 
   - Type : MÃ©trique simple
   - But : Afficher le nombre total de publications classÃ©es comme nÃ©gatives

5. **ğŸ”¥ Langue x Sentiment** 
   - Type : Heatmap
   - But : Analyser visuellement lâ€™intensitÃ© du harcÃ¨lement selon la langue et le sentiment
#

## ğŸ”§ Configuration avancÃ©e

### Variables d'environnement

```bash
# MongoDB
MONGO_HOST=localhost
MONGO_PORT=27017
MONGO_DB=harcelement

# Elasticsearch
ES_HOST=localhost
ES_PORT=9200
ES_INDEX=harcelement_posts

# Logging
LOG_LEVEL=INFO
```

## ğŸš¨ DÃ©pannage

### ProblÃ¨mes courants

1. **Elasticsearch ne dÃ©marre pas**
   ```bash
   # VÃ©rifier les logs
   docker-compose logs elasticsearch
   
   
   ```

2. **Erreur de connexion MongoDB**
   ```bash
   # VÃ©rifier le status
   docker-compose ps mongo
   
   # RedÃ©marrer le service
   docker-compose restart mongo
   ```

3. **ProblÃ¨mes de performance**
   - Augmenter la RAM allouÃ©e Ã  Elasticsearch
   - RÃ©duire la taille des batches de traitement
### Logs et monitoring

```bash
# Voir les logs en temps rÃ©el
docker-compose logs -f

# Logs spÃ©cifiques
docker-compose logs elasticsearch
docker-compose logs kibana
#OR
# View logs
make logs

# Restart services
make down
make up

# Clean everything and restart
make clean
make build
make up
```
## ğŸ› ï¸ Available Make Commands

```bash
make build       # Build Docker images
make up          # Start all services
make down        # Stop all services
make logs        # View logs
make shell       # Access app container
make ingest      # Run data ingestion
make preprocess  # Run preprocessing
make nlp         # Run NLP pipeline
make index       # Index to Elasticsearch
make all         # Run complete pipeline
make clean       # Remove all containers and data
```
## ğŸ“ˆ MÃ©triques de performance

### Benchmarks 

- **Ingestion** : >1000 documents/seconde
- **PrÃ©traitement** : <500 documents/seconde
- **NLP** : <100 documents/seconde
- **Indexation ES** : ~2000 documents/seconde

### Optimisations possibles

1. **ParallÃ©lisation** : Utilisation de multiprocessing
2. **Batch processing** : Traitement par lots plus importants
3. **Caching** : Mise en cache des rÃ©sultats NLP
4. **Index optimization** : Tuning des paramÃ¨tres Elasticsearch


## ğŸ“š Ressources

### Documentation officielle

- [MongoDB Python Driver](https://pymongo.readthedocs.io/)
- [Elasticsearch Python Client](https://elasticsearch-py.readthedocs.io/)
- [Kibana Guide](https://www.elastic.co/guide/en/kibana/current/index.html)
- [NLTK Documentation](https://www.nltk.org/)

### Ressources supplÃ©mentaires

- [Guide NLP avec Python](https://www.nltk.org/book/)
- [Elasticsearch Best Practices](https://www.elastic.co/guide/en/elasticsearch/reference/current/general-recommendations.html)
- [MongoDB Schema Design](https://docs.mongodb.com/manual/data-modeling/)


## ğŸ‘¥ Auteur

- **Ben Slama Latifa** - DÃ©veloppement initial
